import os
import requests
import feedparser
from urllib.parse import quote
from datetime import datetime
from collections import defaultdict
from db_mysql import save_articles, canonicalize_url
from dotenv import load_dotenv
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent
load_dotenv(BASE_DIR / ".env", override=True)

from rules import (
    INCLUDE_SW,
    INCLUDE_SEC_REG_INCIDENT,
    EXCLUDE
)

# -----------------------------
# 1) Article grouping / tagging
# -----------------------------
def group_by_tag_combo(articles: list[dict]) -> dict[tuple[str, ...], list[dict]]:
    grouped = defaultdict(list)

    for a in articles:
        tags = a.get("tags") or []
        # ìˆœì„œë¥¼ ê³ ì •(ë³´ì•ˆ/ê·œì œ/ì‚¬ê³ /SW/ê¸°íƒ€ ìˆœ)
        order = {"ë³´ì•ˆ": 0, "ê·œì œ": 1, "ì‚¬ê³ ": 2, "SW": 3, "ê¸°íƒ€": 4}
        tags_sorted = tuple(sorted(tags, key=lambda x: order.get(x, 99)))

        # íƒœê·¸ì— ë¯¸ í¬í•¨ ì‹œ ê¸°íƒ€ë¡œ
        if not tags_sorted:
            tags_sorted = ("ê¸°íƒ€",)

        grouped[tags_sorted].append(a)

    return grouped


def classify_tags(title: str) -> list[str]:
    t = (title or "").lower()
    tags = []

    if any(x.lower() in t for x in INCLUDE_SW):
        tags.append("SW")

    if any(x.lower() in t for x in INCLUDE_SEC_REG_INCIDENT):
        if any(k in t for k in ["ë³´ì•ˆ", "ì‚¬ì´ë²„", "í•´í‚¹", "ì·¨ì•½ì ", "ê³µê²©", "ëœì„¬ì›¨ì–´"]):
            tags.append("ë³´ì•ˆ")
        elif any(k in t for k in ["unece", "r155", "r156", "iso", "ê·œì œ", "ë²•ê·œ", "ì¸ì¦"]):
            tags.append("ê·œì œ")
        elif any(k in t for k in ["ì‚¬ê³ ", "í™”ì¬", "ë¦¬ì½œ", "ê²°í•¨"]):
            tags.append("ì‚¬ê³ ")
        else:
            tags.append("ê¸°íƒ€")

    return tags


def is_relevant_article(title: str, keyword: str) -> bool:
    t = (title or "").lower()

    # ì œì™¸ í‚¤ì›Œë“œ ë¨¼ì € ê±¸ëŸ¬ëƒ„
    if any(x.lower() in t for x in EXCLUDE):
        return False

    key_tokens = ["ìë™ì°¨sw", "ìë™ì°¨ sw", "ì°¨ëŸ‰ ì†Œí”„íŠ¸ì›¨ì–´", "sdv"]
    tt = t.replace(" ", "")
    if any(k.replace(" ", "") in tt for k in key_tokens):
        return True

    has_sw = any(x.lower().replace(" ", "") in tt for x in INCLUDE_SW)
    has_sec_reg_inc = any(x.lower().replace(" ", "") in tt for x in INCLUDE_SEC_REG_INCIDENT)
    return has_sw or has_sec_reg_inc

# --------------------------------------------------------------
# ë‹¤ì¤‘ í¬í„¸ì—ì„œ ê°™ì€ ê¸°ì‚¬ ì¤‘ë³µ ì œì™¸ (ê°™ì€ ê¸°ì‚¬ ì—¬ëŸ¬ í¬í„¸ì‚¬ì´íŠ¸ ì¶œë ¥)
# --------------------------------------------------------------
import re

def normalize_title(title: str) -> str:
    t = (title or "").strip().lower()
    t = t.replace("ï¼œ", "<").replace("ï¼", ">")
    t = re.sub(r"[â€œâ€\"'â€˜â€™]", "", t)

    # ëì— ë¶™ëŠ” í¬í„¸/ì–¸ë¡ ì‚¬ëª… ì œê±°
    t = re.sub(r"\s*[-|:]\s*[^-|:]{2,30}\s*$", "", t)
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^\wê°€-í£\s]", "", t)

    return t.strip()

def dedup_near_same_title(articles: list[dict]) -> list[dict]:
    seen = set()
    out = []

    for a in articles:
        key = normalize_title(a.get("title", ""))
        if not key:
            continue
        if key in seen:
            continue
        seen.add(key)
        out.append(a)

    return out

# -----------------------------
# 2) RSS crawling
# -----------------------------
def google_news_rss(keyword: str, count: int = 50):
    url = (
        "https://news.google.com/rss/search?"
        f"q={quote(keyword)}&hl=ko&gl=KR&ceid=KR:ko"
    )
    feed = feedparser.parse(url)

    articles = []
    for entry in feed.entries[:count]:
        articles.append({
            "title": entry.title,
            "url": entry.link,
            "published": getattr(entry, "published", "")
        })
    return articles


# -----------------------------
# 3) Slack BOT (API) posting
# -----------------------------
def slack_post_message(text: str) -> str:
    """ì±„ë„ì— ë©”ì¸ ë©”ì‹œì§€ë¥¼ ì˜¬ë¦¬ê³  ts ë°˜í™˜"""
    token = os.environ.get("SLACK_BOT_TOKEN")
    channel = os.environ.get("SLACK_CHANNEL_ID")
    if not token or not channel:
        raise RuntimeError("SLACK_BOT_TOKEN / SLACK_CHANNEL_ID í™˜ê²½ë³€ìˆ˜ ë¯¸ ì¡´ì¬.")

    #ì§€ì •í•œ SLACK_CHANNEL_ID ì™€ BOT_TOKEN ìœ¼ë¡œ íƒ‘ 3ê°œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ URLì€ ì“°ë ˆë“œ ì²˜ë¦¬
    api_url = "https://slack.com/api/chat.postMessage"
    headers = {"Authorization": f"Bearer {token}"}
    payload = {
        "channel": channel,
        "text": text,
        "unfurl_links": False,
        "unfurl_media": False,
    }

    r = requests.post(api_url, headers=headers, json=payload, timeout=10)
    r.raise_for_status()
    data = r.json()

    print("Slack API(main) ok:", data.get("ok"), "error:", data.get("error"))
    if not data.get("ok"):
        raise RuntimeError(f"Slack API ì‹¤íŒ¨: {data.get('error')}")

    return data["ts"]


def slack_post_thread(text: str, thread_ts: str) -> None:
    """ë©”ì¸ ë©”ì‹œì§€ì˜ ìŠ¤ë ˆë“œ(ëŒ“ê¸€)ë¡œ ì¶”ê°€ ë©”ì‹œì§€ ì˜¬ë¦¬ê¸°"""
    token = os.environ.get("SLACK_BOT_TOKEN")
    channel = os.environ.get("SLACK_CHANNEL_ID")
    if not token or not channel:
        raise RuntimeError("SLACK_BOT_TOKEN / SLACK_CHANNEL_ID í™˜ê²½ë³€ìˆ˜ ë¯¸ ì¡´ì¬.")

    api_url = "https://slack.com/api/chat.postMessage"
    headers = {"Authorization": f"Bearer {token}"}
    payload = {
        "channel": channel,
        "text": text,
        "thread_ts": thread_ts,
        "unfurl_links": False,
        "unfurl_media": False,
    }

    r = requests.post(api_url, headers=headers, json=payload, timeout=10)
    r.raise_for_status()
    data = r.json() 

    print("Slack API(thread) ok:", data.get("ok"), "error:", data.get("error"))
    if not data.get("ok"):
        raise RuntimeError(f"Slack API(thread) ì‹¤íŒ¨: {data.get('error')}")

# -----------------------------
# 4) Message building
# -----------------------------
def _tag_label(a: dict) -> str:
    tags = a.get("tags", []) or []
    order = {"ë³´ì•ˆ": 0, "ê·œì œ": 1, "ì‚¬ê³ ": 2, "SW": 3, "ê¸°íƒ€": 4}
    tags = sorted(set(tags), key=lambda x: order.get(x, 99))
    return "".join(f"[{t}]" for t in tags)

def article_key(a: dict) -> str:   #ê¸°ì‚¬ URL ì¤‘ë³µ ì œê±°ìš© í‚¤
    url = (a.get("url") or "").strip().lower()
    title = (a.get("title") or "").strip().lower()
    return url or title  # url ìˆìœ¼ë©´ url ìš°ì„ , ì—†ìœ¼ë©´ title

def make_message(keyword: str, articles: list[dict], max_per_group: int = 3):
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    grouped = group_by_tag_combo(articles)

    order = {"ë³´ì•ˆ": 0, "ê·œì œ": 1, "ì‚¬ê³ ": 2, "SW": 3, "ê¸°íƒ€": 4}
    def combo_sort_key(combo):
        return (order.get(combo[0], 99), len(combo), combo)

    combos = sorted(grouped.keys(), key=combo_sort_key)

    lines = [
        "ğŸ“Œ *Daily Auto SW News*",
        f"- í‚¤ì›Œë“œ: *{keyword_title}*",
        f"- ì‹œê°„: {now}",
        f"- ì‹ ê·œ ê¸°ì‚¬: {len(articles)}ê±´",
        ""
    ]
    shown_keys = set()

    for combo in combos:
        items = grouped[combo]

        # ì •ë ¬ ê¸°ì¤€ì„ ê³ ì • 
        items = sorted(
            items,
            key=lambda a: ((a.get("published") or ""), (a.get("title") or "")),
            reverse=True
        )

        header = "".join([f"[{t}]" for t in combo])
        lines.append(f"*{header}* ({len(items)})")

        main_items = items[:max_per_group]
        for i, a in enumerate(main_items, 1):
            shown_keys.add(article_key(a))
            title = (a.get("title") or "").replace("<", "ï¼œ").replace(">", "ï¼")
            url = (a.get("url") or "").strip()
            lines.append(f"{i}. <{url}|{title}>")

        if len(items) > max_per_group:
            lines.append(f"â€¦ ì™¸ {len(items) - max_per_group}ê±´ (ìŠ¤ë ˆë“œ ì°¸ê³ )")

        lines.append("")

    return "\n".join(lines), shown_keys

def make_thread_message(articles: list[dict], shown_keys: set, max_per_group: int = 3) -> str:
    grouped = group_by_tag_combo(articles)
    order = {"ë³´ì•ˆ": 0, "ê·œì œ": 1, "ì‚¬ê³ ": 2, "SW": 3, "ê¸°íƒ€": 4}

    def combo_sort_key(combo):
        return (order.get(combo[0], 99), len(combo), combo)

    combos = sorted(grouped.keys(), key=combo_sort_key)

    lines = ["*ìƒì„¸ ê¸°ì‚¬ ëª©ë¡(ì™¸ nê±´)*", ""]
    has_any = False

    for combo in combos:
        items = grouped[combo]

        # make_messageì™€ ë™ì¼í•œ ì •ë ¬
        items = sorted(
            items,
            key=lambda a: ((a.get("published") or ""), (a.get("title") or "")),
            reverse=True
        )

        # ë©”ì¸ì— ë‚˜ì˜¨ ê±´ ì œì™¸ (ë©”ì¸ì— ë³´ì—¬ì£¼ëŠ” ê¸°ì‚¬ì™€ ì“°ë ˆë“œ ëŒ“ê¸€ ì¤‘ë³µ ë°©ì§€)
        rest = [a for a in items if article_key(a) not in shown_keys]
        if not rest:
            continue

        has_any = True
        header = "".join([f"[{t}]" for t in combo])
        lines.append(f"*{header} ì¶”ê°€ {len(rest)}ê±´*")

        for i, a in enumerate(rest, 1):
            title = (a.get("title") or "").replace("<", "ï¼œ").replace(">", "ï¼")
            url = (a.get("url") or "").strip()
            lines.append(f"{i}. <{url}|{title}>")

        lines.append("")

    return "\n".join(lines) if has_any else ""


# -----------------------------
# 5) main
# -----------------------------
if __name__ == "__main__":
    import sys

    keyword = '("ìë™ì°¨ SW" OR ìë™ì°¨SW OR "ì°¨ëŸ‰ ì†Œí”„íŠ¸ì›¨ì–´" OR SDV)'
    keyword_title = ("ìë™ì°¨ SW Â· ìë™ì°¨SW Â· ì°¨ëŸ‰ ì†Œí”„íŠ¸ì›¨ì–´ Â· SDV")

    MAX_PER_TAG = 5

    print("SCRIPT:", __file__)
    print("PY:", sys.executable)

    # ì“°ë ˆë“œ í† í°ì— ë“¤ì–´ê°„ í™˜ê²½ë³€ìˆ˜
    token = os.environ.get("SLACK_BOT_TOKEN")
    channel = os.environ.get("SLACK_CHANNEL_ID")
    print("BOT_TOKEN:", (token[:10] + "...") if token else None)
    print("CHANNEL_ID:", channel)

    raw_articles = google_news_rss(keyword, count=50)
    articles = []

for a in raw_articles:
    if is_relevant_article(a["title"], keyword):
        a["tags"] = classify_tags(a["title"])
        articles.append(a)


    print(f"raw={len(raw_articles)} filtered={len(articles)}")

    # (ì¶”ê°€) run ë‚´ URL ì¤‘ë³µ ì œê±° (canonical ê¸°ì¤€) ->
    seen = set()
    deduped = []
    for a in articles:
        key = canonicalize_url(a.get("url", ""))
        if not key or key in seen:
            continue
        seen.add(key)
        deduped.append(a)
    articles = deduped
    print(f"after_deduped={len(articles)}")

# -----------------------------
# 6) DATABASE
# -----------------------------

if not articles:
        print("not filter")
else:
        # DB ì €ì¥ (ì¤‘ë³µURL ê±¸ëŸ¬ë‚´ê¸° ìœ„í•¨)
        inserted, skipped, new_articles = save_articles(articles, keyword)
        print(f"DB ì €ì¥ ê²°ê³¼: ì‹ ê·œ {inserted}, ì¤‘ë³µ {skipped}")
        new_articles = dedup_near_same_title(new_articles)

        # ì‹ ê·œê°€ ì—†ìœ¼ë©´ Slack ì•ˆ ë³´ëƒ„ (ë„ë°° ë°©ì§€)
        if not new_articles:
            print("ì‹ ê·œ ê¸°ì‚¬ ì—†ìŒ â†’ Slack ì „ì†¡ ìŠ¤í‚µ")
        else:
            # 1) ë©”ì¸ ë©”ì‹œì§€(ì‹ ê·œ ê¸°ì¤€)
            main_msg, shown_keys = make_message(keyword, new_articles, max_per_group=MAX_PER_TAG)
            thread_ts = slack_post_message(main_msg)

            thread_msg = make_thread_message(new_articles, shown_keys, max_per_group=MAX_PER_TAG)
            if thread_msg:
                slack_post_thread(thread_msg, thread_ts)
                
            print(f"í‚¤ì›Œë“œ: {keyword_title}")
            print("Slack sending SUCCESS")